Implementation Plan: Module Breakdown

Self-Learning Browser Agent via Deployment-Time Self-Distillation

⸻

0. Orchestration Layer (Glue Code)

0.1 Agent Runtime Controller

Responsibility
	•	Coordinates perception → policy → action → learning loop
	•	Manages phase transitions (demo / inference / adaptation)

Inputs
	•	Task specification
	•	Runtime configuration

Outputs
	•	Executed browser actions
	•	Logs and metrics

Notes
	•	Keep this boring and explicit
	•	Central place to enforce safety rules

MVP
	•	Simple state machine

⸻

1. Browser Interaction Layer

1.1 Playwright Session Manager

Responsibility
	•	Launch browser
	•	Manage sessions and state
	•	Reset / replay environments

Inputs
	•	URL
	•	Credentials (mocked or env)

Outputs
	•	Browser context
	•	Page handles

MVP
	•	Single browser instance
	•	Deterministic replay

⸻

1.2 Action Executor

Responsibility
	•	Execute high-level actions:
	•	click
	•	type
	•	scroll
	•	wait

Inputs
	•	Action specification
	•	Target selector or coordinates

Outputs
	•	Success / failure
	•	Execution metadata

Notes
	•	Must be robust to partial failures
	•	Log everything

MVP
	•	Limited action set
	•	Clear error handling

⸻

2. Observation & Logging Layer

2.1 DOM Snapshotter

Responsibility
	•	Capture structured DOM state

Inputs
	•	Page object

Outputs
	•	Serialized DOM tree
	•	Element attributes

MVP
	•	Pruned DOM
	•	Stable ordering

⸻

2.2 Visual Capture Module

Responsibility
	•	Capture screenshots
	•	Extract element bounding boxes

Inputs
	•	Page object

Outputs
	•	RGB images
	•	Element → bbox mapping

Notes
	•	Keep resolution reasonable
	•	Deterministic cropping

⸻

2.3 Trajectory Logger

Responsibility
	•	Record full multimodal trajectories

Inputs
	•	Observations
	•	Actions
	•	Outcomes

Outputs
	•	Replayable trajectory objects

MVP
	•	JSON + image blobs
	•	Timestamped

⸻

3. Multimodal Perception & Encoding

3.1 DOM Encoder

Responsibility
	•	Encode DOM structure into model-ready format

Inputs
	•	Serialized DOM

Outputs
	•	Tokenized / embedded DOM representation

MVP
	•	Simple linearization
	•	Role + text + hierarchy

⸻

3.2 Vision Encoder

Responsibility
	•	Encode screenshots

Inputs
	•	RGB images

Outputs
	•	Visual embeddings

Notes
	•	Pretrained model
	•	Frozen weights

⸻

3.3 Observation Fusion Module

Responsibility
	•	Combine DOM + vision + history

Inputs
	•	DOM embeddings
	•	Visual embeddings
	•	Action history

Outputs
	•	Unified observation tensor

MVP
	•	Concatenation + projection
	•	No fancy attention yet

⸻

4. Policy & Decision-Making

4.1 Multimodal Policy Backbone (Frozen)

Responsibility
	•	Map observations → action distributions

Inputs
	•	Fused observation

Outputs
	•	Action logits
	•	Confidence / entropy

Notes
	•	This is where GPU inference happens
	•	Keep frozen for stability

⸻

4.2 Action Head / Adapter Layer (Learnable)

Responsibility
	•	Adapt behavior without touching backbone

Inputs
	•	Backbone outputs

Outputs
	•	Final action probabilities

MVP
	•	LoRA or MLP head
	•	Small parameter count

⸻

4.3 Action Selector

Responsibility
	•	Choose action to execute

Inputs
	•	Action distribution
	•	Confidence thresholds

Outputs
	•	Concrete browser action

Notes
	•	Deterministic mode for demo helps

⸻

5. Self-Distillation Fine-Tuning (SDFT)

5.1 Teacher Policy Manager

Responsibility
	•	Maintain EMA teacher

Inputs
	•	Student parameters

Outputs
	•	Teacher predictions

MVP
	•	Simple EMA update
	•	No gradients

⸻

5.2 Confidence & Success Gating

Responsibility
	•	Decide when learning is allowed

Inputs
	•	Entropy
	•	Action success
	•	Task progress

Outputs
	•	Binary learn / don’t learn signal

This is critical
	•	This is your “safety story”

⸻

5.3 Distillation Loss Module

Responsibility
	•	Compute self-distillation loss

Inputs
	•	Teacher logits
	•	Student logits

Outputs
	•	Scalar loss

MVP
	•	KL divergence

⸻

5.4 Online Optimizer

Responsibility
	•	Apply bounded updates

Inputs
	•	Loss
	•	Update budget

Outputs
	•	Updated adapter parameters

Notes
	•	Very small learning rate
	•	Few steps per update

⸻

6. Safety, Stability & Control

6.1 Update Budget Manager

Responsibility
	•	Enforce limits on learning

Inputs
	•	Update count
	•	Norms

Outputs
	•	Allow / deny update

⸻

6.2 Checkpoint & Rollback System

Responsibility
	•	Save stable states
	•	Restore on failure

Inputs
	•	Model state
	•	Performance metrics

Outputs
	•	Restored parameters

MVP
	•	Single last-good checkpoint

⸻

6.3 Shadow Evaluation (Optional but impressive)

Responsibility
	•	Compare adapted vs frozen policy

Inputs
	•	Parallel predictions

Outputs
	•	Divergence metrics

⸻

7. Task & Evaluation Layer

7.1 Task Specification Module

Responsibility
	•	Define goals and success criteria

Inputs
	•	User intent

Outputs
	•	Completion signals

⸻

7.2 Metric Tracker

Responsibility
	•	Track:
	•	success rate
	•	retries
	•	time-to-complete

Outputs
	•	Live metrics for demo

⸻

8. UI / Visualization (Demo Gold)

8.1 Live Dashboard

Responsibility
	•	Show:
	•	current action
	•	confidence
	•	learning on/off
	•	rollback events

MVP
	•	Console + overlay
	•	Simple graphs

⸻

9. Infrastructure & Dev Experience

9.1 Configuration System

Responsibility
	•	Centralize hyperparameters

MVP
	•	YAML or JSON

⸻

9.2 Reproducibility Harness

Responsibility
	•	Run demo end-to-end with one command

MVP
	•	Shell script or Makefile

⸻

10. Submission Artifacts

10.1 Demo Script

Responsibility
	•	Deterministic demo run

⸻

10.2 README & Explainer

Responsibility
	•	Tell the story clearly

⸻

Final Build Order (Recommended)
	1.	Playwright + logging
	2.	Static policy inference
	3.	One-shot demo replay
	4.	SDFT loop
	5.	Safety + rollback
	6.	Visualization
	7.	Polish

⸻

Bottom Line

This plan is:
	•	implementable
	•	contest-scoped
	•	technically serious
	•	demo-friendly

Nothing here is fluff — each module justifies itself.

If you want, next we can:
	•	trim this into an MVP-only version
	•	convert this into a 2-week build plan
	•	or design the repo structure

You’re doing exactly the right thing.