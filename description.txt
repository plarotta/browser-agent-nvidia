High-Level System Design

Self-Learning Browser Agent via Deployment-Time Self-Distillation

⸻

1. System Overview

The system is a browser automation agent that learns how a specific user performs a task by observing a single demonstration, then adapts during deployment using Self-Distillation Fine-Tuning (SDFT) to remain robust to UI and DOM changes — without labels or retraining.

Core principle:

Frozen general intelligence + bounded personalization at the edges

⸻

2. Architecture Diagram (conceptual)

┌────────────────────────┐
│   User Demonstration   │
│ (Playwright Session)   │
└───────────┬────────────┘
            │
┌───────────▼────────────┐
│   Observation Encoder  │
│  (DOM + Screenshot)    │
└───────────┬────────────┘
            │
┌───────────▼────────────┐
│  Multimodal Policy     │◄──────────┐
│  (Frozen Backbone)     │           │
└───────────┬────────────┘           │
            │                         │
┌───────────▼────────────┐   ┌───────▼────────┐
│   Action Head / LoRA   │   │ EMA Teacher     │
│  (Adaptable Surface)  │   │ (Frozen Copy)   │
└───────────┬────────────┘   └────────────────┘
            │
┌───────────▼────────────┐
│  Execution Engine      │
│ (Playwright Controller)│
└───────────┬────────────┘
            │
┌───────────▼────────────┐
│   SDFT Update Module   │
│ (Deployment-Time)     │
└────────────────────────┘


⸻

3. Core Components

3.1 Execution Engine (Playwright)

Responsibilities
	•	Execute browser actions
	•	Capture:
	•	DOM snapshots
	•	screenshots
	•	action traces
	•	Provide deterministic replay

Why Playwright
	•	Reliable automation
	•	Clean action abstraction
	•	Easy logging and rollback
	•	Competition-friendly (no browser plugin friction)

⸻

3.2 Observation Encoder (Multimodal)

Each timestep produces a structured observation:

{
  "dom_tree": "...",
  "screenshot": "RGB image",
  "element_bboxes": [...],
  "previous_action": "...",
  "task_context": "..."
}

Purpose
	•	DOM captures semantics
	•	Screenshots capture layout & visual affordances
	•	Together → robustness to web drift

⸻

3.3 Multimodal Policy Model

Inputs
	•	Encoded DOM structure
	•	Screenshot embeddings
	•	Task description
	•	Action history

Outputs
	•	Next browser action:
	•	click(selector)
	•	type(text)
	•	scroll
	•	wait
	•	Confidence / entropy score

Design Choice
	•	Backbone is frozen
	•	Only a small adaptation surface is learnable

This keeps behavior stable and GPU-efficient.

⸻

3.4 Adaptation Surface (LoRA / Action Head)

What adapts
	•	Action selection logits
	•	Selector ranking
	•	Attention weighting over DOM vs vision

What does NOT adapt
	•	Vision encoder
	•	Language encoder
	•	Core reasoning layers

This is critical for:
	•	safety
	•	preventing catastrophic forgetting
	•	fast updates

⸻

3.5 Self-Distillation Fine-Tuning (SDFT) Module

This runs during deployment, not offline.

Teacher
	•	EMA copy of the policy
	•	Updated slowly
	•	Never directly optimized

Student
	•	Current policy
	•	Learns from:
	•	high-confidence actions
	•	successful trajectories

Update Rule (conceptual)
	•	Minimize KL divergence:

KL(teacher(action | state) || student(action | state))


	•	Only when:
	•	entropy < threshold
	•	action succeeds
	•	update budget available

⸻

3.6 Safety & Stability Controls (Critical)

To avoid drift:
	•	Confidence gating
	•	No learning from uncertain steps
	•	Update budget
	•	Max steps / max norm
	•	Rollback
	•	Restore last stable checkpoint
	•	Shadow evaluation
	•	Compare adapted vs frozen policy

This is the key innovation from a deployment perspective.

⸻

4. Learning Lifecycle

Phase 1: Demonstration
	•	User completes task once
	•	System records full multimodal trajectory
	•	No learning yet

Phase 2: Initial Imitation
	•	Agent attempts task
	•	Uses demonstration as reference
	•	Logs successes and failures

Phase 3: Deployment Adaptation (SDFT)
	•	As environment changes:
	•	DOM shifts
	•	UI changes
	•	Agent:
	•	self-distills from successful behavior
	•	updates only the adaptation surface

Phase 4: Recovery & Rollback
	•	If performance degrades:
	•	rollback instantly
	•	continue execution safely

⸻

5. GPU Utilization (Explicit for NVIDIA)

GPU is used for:
	•	multimodal inference (vision + language)
	•	batched policy evaluation
	•	online self-distillation updates
	•	EMA teacher maintenance

This is:
	•	low-latency
	•	parallelizable
	•	production-realistic

⸻

6. Demo Use Case (Concrete)

Task

Log into a site → navigate to invoices → download latest PDF

Demonstrated
	•	One-shot learning
	•	Website UI change
	•	Failure of static agent
	•	Recovery via SDFT
	•	Rollback safety

⸻

7. Why This Design Is Compelling
	•	Real deployment problem
	•	No labels
	•	No retraining
	•	Bounded, reversible learning
	•	Multimodal
	•	GPU-accelerated
	•	Generalizes beyond browsers

This is infrastructure for agents, not a toy automation script.

⸻

8. One-Sentence Summary (submission-ready)

We present a browser agent that safely adapts during deployment by self-distilling from its own successful behavior, enabling personalized automation without retraining or supervision.
