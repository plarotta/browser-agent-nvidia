Self-Learning Browser Agent
MVP Specification
For Coding Agent Implementation
1. Product Overview
A browser agent that runs locally and learns to perform simple web tasks by watching the user do them. The user demonstrates a task by performing it normally; the agent observes, learns, and can then replay the task autonomously.
1.1 Why This Exists
* Current agentic approaches rely on APIs and MCP servers, which are brittle and may not exist for many web apps.
* Setting up API keys and MCP connections is too involved for non-technical users.
* Running expensive frontier models for every agentic step is cost-prohibitive for consumers.
* Sensitive/private information is sent to model APIs; local inference avoids this.
* LLMs still struggle with visual browser control. Adding a Google Calendar event is a few clicks for a human but requires visual reasoning, DOM understanding, and multi-step planning for an agent. No agentic framework does this well today.
1.2 Core Value Proposition
Show the agent what to do by doing it yourself. The agent learns from your demonstration and can repeat the task. No API keys, no MCP setup, no coding required.
________________


2. System Architecture
2.1 Infrastructure
Component
	Role
	Details
	Mac M4 (24GB)
	Host machine
	Runs the browser agent, serves the base model via MLX, hosts LoRA adapters, and provides the main app interface.
	RunPod GPU Instance
	Remote training server
	Hosts the same base model for training. Receives trajectory uploads, runs SDFT LoRA training, and sends adapters back to Mac. Needs sufficient VRAM to host Gemma 12B + LoRA training overhead.
	NVIDIA NIM API
	Demonstration enrichment
	Hosted VLM endpoint used to enrich raw demonstrations with rich explanations before training. Uses meta/llama-3.2-90b-vision-instruct on build.nvidia.com.
	2.2 Models
Model
	Purpose
	Where It Runs
	gemma-3-12b-it-qat-4bit (MLX)
	Base brain: inference + student model for SDFT
	Mac M4 via MLX (local inference), RunPod (training)
	meta/llama-3.2-90b-vision-instruct
	Demonstration enrichment teacher
	NVIDIA NIM API (build.nvidia.com). Upgrade from the 11B variant for much stronger trajectory analysis.
	LoRA adapters (per-task)
	Task-specific fine-tuned weights
	Trained on RunPod, deployed on Mac M4 via hot-swap
	3. Data Pipeline
3.1 Demonstration Recording
The user records a demonstration by performing the task in a Playwright-controlled browser. The system captures a trajectory consisting of:
* Screenshots at each step
* DOM snapshots (interactive elements + visible page text via innerText)
* User actions (clicks, typing, navigation)
* Thinking steps (if applicable)
Command: uv run python -m src.main record --url <url> --task <task_name>
Output: Trajectory saved to logs/{task_name}_run/ containing screenshots, DOM snapshots, and action log.
3.2 Demonstration Enrichment
Raw demonstrations are enriched by the 90B NIM VLM before training. For each step in the trajectory, the enrichment model receives the screenshot + action and produces:
* An explanation of why the action is correct given the page context
* Element rationale (why this element, not another)
* Expected outcome of the action
* Thinking steps that a model should follow
Enrichment model: meta/llama-3.2-90b-vision-instruct via NVIDIA NIM API (build.nvidia.com). This is a major upgrade from the 11B model previously used. Enrichment quality is the bottleneck for SDFT, and the 90B model provides significantly richer analysis of browser trajectories.
Fallback: If the NIM API key is missing or a call fails, training falls back to raw action JSON seamlessly.
Caching: Enrichments should be cached so that retraining does not re-enrich the same demonstration.
________________


4. Training Pipeline (SDFT)
4.1 What SDFT Is
Self-Distillation Fine-Tuning (SDFT) is an on-policy distillation method from the paper "Self-Distillation Enables Continual Learning" (Shenfeld et al., 2026). It enables learning from demonstrations without catastrophic forgetting of the base model's capabilities. This is critical: we need the model to learn new browser tasks without losing its general reasoning ability.
4.2 How SDFT Works
The same model plays two roles:
1. Student = the model conditioned only on the query (e.g., the current page state + goal). It generates an on-policy rollout and produces token logits.
2. Teacher = the same model conditioned on the query PLUS the enriched expert demonstration. Because it has the answer in context, it produces better logits. Crucially, since it is the same model with extra context, its output distribution stays close to the base model.
3. Training = minimize reverse KL divergence between student and teacher token distributions. The gradient is computed on tokens the student itself sampled (on-policy).
The key insight: unlike SFT which forces the model to match arbitrary target distributions (causing catastrophic forgetting), SDFT's teacher distribution is inherently close to the base model, so updates are gentle and preserve prior capabilities.
4.3 LoRA Integration
The SDFT paper uses full fine-tuning. Our implementation uses LoRA adapters instead:
* The LoRA adapter IS the student parameters. Base Gemma weights are frozen; only adapter weights get gradient updates.
* Student forward pass: base Gemma + current LoRA adapter, conditioned on query only.
* Teacher forward pass: base Gemma + EMA copy of LoRA adapter, conditioned on query + enriched demonstration.
* EMA update: ema_adapter = alpha * student_adapter + (1 - alpha) * ema_adapter.
4.4 Training Loop (Per Demonstration)
1. Upload trajectory tar.gz to RunPod server via /upload_trajectory endpoint.
2. Server executes enrichment of each trajectory step via NIM 90B API (or uses cached enrichments).
3. For each training step:
  a. Sample a query (page state + goal) from the trajectory.
  b. Student generates an on-policy rollout (tokens sampled from student distribution).
  c. Teacher produces logits for the same tokens, conditioned on query + enriched demo.
  d. Compute reverse KL loss between student and teacher distributions.
  e. Backpropagate through LoRA adapter weights only.
  f. Update EMA teacher adapter.
4. Repeat until convergence.
5. LoRA adapter is saved and sent back to Mac.
6. Mac hot-swaps the adapter into the local MLX inference engine.
4.5 Training Configuration
Parameter
	Default
	Notes
	Model
	gemma-3-12b-it-qat-4bit
	Same model for student and teacher (EMA)
	LoRA rank
	16
	

	Learning rate
	1e-5
	

	Epochs
	2
	SDFT benefits from multiple epochs (paper finding)
	EMA alpha
	0.02
	IMPORTANT: Consider increasing to 0.1-0.3 for low-data regime (5-10 demos). Default 0.02 is very slow teacher update, teacher will barely move from base.
	KL estimator
	Full analytic per-token
	Best stability per paper ablations (Appendix A.1). Biased but stable.
	Rollouts per prompt
	1
	Paper found no gain from multiple rollouts
	________________


5. Inference Pipeline
At runtime, the agent loops through these steps until FINISH or max_steps:
1. Observe: Extract interactive DOM elements (with stable data-agent-ids) and visible page text via innerText, plus a screenshot.
2. Build prompt: Goal + output format + rules + action history + DOM summary. Format instructions come first (truncation-safe for small models).
3. Generate: VLM produces a JSON action via the base model + active LoRA adapter. Action types: TYPE, CLICK, PRESS_ENTER, SCROLL, WAIT, NAVIGATE, or FINISH.
4. Parse: Primary JSON parser, fallback regex parser (line-start anchored), and done-language detection.
5. Apply guards: Programmatic guards prevent common small-model mistakes (e.g., if model tries to FINISH without submitting a search, runtime overrides to PRESS_ENTER).
6. Execute: Action executed via Playwright. PRESS_ENTER waits for page load. TYPE rejects empty values.
7. Terminate: When model outputs FINISH (with answer in value), or at max_steps (default 15).
5.1 Backends
Backend
	Flag
	Use Case
	MLX
	--backend mlx
	Primary. Local inference on Mac M4. Uses gemma-3-12b-it-qat-4bit by default.
	Remote vLLM
	--backend remote_vllm
	Browser on Mac, model on RunPod GPU box. Set --server-url or VLLM_SERVER_URL env var.
	Transformers
	--backend transformers
	CUDA/CPU fallback
	TensorRT
	--backend tensorrt
	Optimized NVIDIA inference
	NIM
	--backend nim
	NVIDIA NIM API
	6. Remote Server (RunPod)
The RunPod GPU server exposes a FastAPI control plane with these endpoints:
Endpoint
	Method
	Purpose
	/health
	GET
	Health check, vLLM status, active adapter
	/act
	POST
	Inference (prompt + screenshot)
	/upload_trajectory
	POST
	Upload trajectory tar.gz for training
	/train
	POST
	Trigger SDFT training with PEFT LoRA
	/train/status
	GET
	Check training job progress
	/adapters
	GET
	List available LoRA adapters
	/adapters/{name}/load
	POST
	Hot-load adapter into vLLM
	/adapters/{name}/unload
	POST
	Unload adapter from vLLM
	Launch: ./scripts/start_server.sh nvidia/Nemotron-Nano-12B-v2-VL-BF16
________________


7. Critical Implementation Notes
7.1 Training Loop (UNTESTED - Top Priority)
The training loop has not been tested end-to-end yet. This is the highest priority item. Before optimizing anything else, verify the full round-trip:
1. Record a trivial demo (e.g., a Google search).
2. Enrich it via NIM 90B API.
3. Upload to RunPod and trigger training.
4. Download the resulting LoRA adapter.
5. Hot-swap into local MLX inference.
6. Run inference with the adapter and verify behavior changed.
Do not debug training quality and adapter loading simultaneously. Prove the pipeline works first, then tune.
7.2 KL Loss Sanity Check
Log the KL loss at step 0. If it is near zero, the teacher and student logits are too similar and training will not produce meaningful learning. This can happen because:
* The enrichment is too weak (11B model produced generic narration instead of insightful analysis; the upgrade to 90B should help).
* The 4-bit quantization of Gemma degrades in-context learning quality. The SDFT paper shows SDFT underperforms SFT at 3B params because ICL is too weak. 12B quantized is in the borderline zone.
* The EMA alpha is too low (0.02 default means teacher barely moves from base). Try 0.1-0.3 for few-shot scenarios.
7.3 NIM Enrichment
* Switch enrichment model from llama-3.2-11b-vision-instruct to meta/llama-3.2-90b-vision-instruct on build.nvidia.com.
* Each demo step hits the API. For a 15-step trajectory, that is 15 API calls. Test the failure/timeout path explicitly.
* Cache enrichments to avoid re-enriching on retrain.
* If free tier rate limits become an issue, consider self-hosting a smaller model on RunPod as fallback.
7.4 Learned Artifacts Warning
The SDFT paper notes a subtle failure mode: the student can inherit spurious linguistic patterns from the teacher like "Based on the text..." or "Following the example..." because the teacher was conditioned on demonstrations. The paper recommends masking the loss over the first few tokens during training to suppress these artifacts.
7.5 Adapter Hot-Swap
The system needs a mechanism to select the correct LoRA adapter at inference time. Two options:
* Manual: user selects which task to run (simpler, fine for MVP).
* Automatic: a lightweight task classifier routes to the right adapter (future work).
7.6 Observation Strategy
The current implementation uses DOM snapshots (interactive elements + visible text) plus screenshots. For a 12B quantized model, consider whether a hybrid approach (screenshots + simplified DOM or element highlighting/bounding boxes) would improve reliability. Pure vision-based browser control is still very hard even for frontier models.
________________


8. Competition Demos (3-5 Required)
Demos should demonstrate a spectrum from simple to impressive. Prioritize demos 3 and 5 for maximum wow factor.
#
	Demo
	Why It Works
	Complexity
	1
	Block out time on Google Calendar
	Simple, relatable, clear start-to-finish task
	Low
	2
	Find the cheapest flight
	Multi-step reasoning, comparison, multiple sites
	High
	3
	Fill out a repetitive form (expense reports, job applications)
	Everyone hates this. Strong "I need this" reaction.
	Medium
	4
	Monitor a product page and notify when price drops below $X
	Shows the agent can do things you cannot easily do yourself.
	Medium
	5
	Cross-app workflow: extract meeting details from an email, add to calendar
	Shows composition across apps, which is the killer feature.
	High
	Recommended lead demo: #3 or #5. Calendar blocking is too simple to wow. Flight search is impressive but people think "I can just use Google Flights."
9. Existing Project Structure
Path
	Purpose
	src/agent/
	Agent runtime (step loop, prompt building, guards), action parser
	src/browser_interaction/
	Playwright session manager, action executor
	src/observation/
	DOM snapshotter (elements + page text), visual capture
	src/policy/
	Multimodal policy dispatch, backend implementations (Transformers, TensorRT, MLX, NIM, Remote vLLM)
	src/server/
	FastAPI control plane (api.py), LoRA adapter manager, SDFT trainer worker
	src/shared/
	Pydantic request/response schemas shared between client and server
	src/sdft/
	Self-distillation: EMA teacher/gating (sdft_module.py), MLX SDFT trainer with NIM-enriched demos (sdft_trainer_mlx.py)
	src/safety/
	Checkpoint save/load/rollback
	src/evaluation/
	Goal spec, success checker (offline evaluation)
	src/utils/
	Config, trajectory logger, trajectory uploader
	scripts/
	Server launch scripts (RunPod)
	10. Implementation Priority Order
1. Test the full training round-trip end-to-end. Record a simple demo, enrich via 90B NIM, train on RunPod, download adapter, hot-swap into local MLX, verify inference behavior changed. This is the only untested piece.
2. Switch enrichment model to 90B. Change the NIM API endpoint from llama-3.2-11b-vision-instruct to meta/llama-3.2-90b-vision-instruct. Same API format, just swap the model string.
3. Add KL loss logging at step 0. If near zero, training will not learn. Diagnose whether enrichment quality, quantization, or EMA alpha is the cause.
4. Cache enrichments. Avoid re-enriching the same demo on retrain. Store enriched outputs alongside trajectories.
5. Record clean trajectories for all 3-5 competition demos. Do this before optimizing training, because you need to know what the trajectories look like.
6. Tune training for your best demo first. Get one task working well, then generalize. Experiment with EMA alpha (try 0.1-0.3 instead of 0.02 for few-shot).
7. Build the presentation flow. Recording -> training -> execution should feel seamless for the demo.
11. Open Questions
* What is the training turnaround time? User records demo -> sends to RunPod -> gets adapter back. Target: minutes, not hours.
* How does the agent handle task failure / recovery? If it clicks the wrong thing mid-task, can it recover or does it spiral?
* How many demonstrations does a user need per task before the adapter is useful? 5-10 is reasonable; 50 is too many.
* What tokens/sec does Gemma 12B QAT 4-bit achieve on M4? If 3-4 seconds per action step and 15 steps per task, the user waits a full minute. Value prop shifts from "saves time" to "runs in background."
* Should adapter selection be manual (user picks task) or automatic (task classifier) for MVP? Manual is simpler.